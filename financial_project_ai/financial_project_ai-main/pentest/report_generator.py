# pentest/report_generator.py
from datetime import datetime
from typing import Dict, List, Tuple
import json
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class PentestReportGenerator:
    """G√©n√©rateur de rapports de pentesting avec sortie console en MD et JSON"""
    
    @staticmethod
    def generate_console_output_section(console_output: str) -> str:
        """
        G√©n√©rer la section de sortie console (Markdown)
        
        Args:
            console_output: Sortie console captur√©e
        
        Returns:
            str: Section format√©e
        """
        return f"""## üìü Complete Console Output

This section contains the complete, unmodified output from the penetration testing suite execution.
All tests, results, and system messages are captured below in chronological order.

---

### Execution Log
```
{console_output}
```

---

**Note:** This log provides a detailed trace of all security tests performed, including:
- Test initialization and configuration
- Real-time test execution status
- Vulnerability detection alerts
- Pass/fail results for each test
- Error messages and warnings
- Final statistics and recommendations

"""
    
    @staticmethod
    def _calculate_statistics(results: Dict) -> Dict:
        """Calculer les statistiques globales"""
        stats = {
            'total_vulnerabilities': 0,
            'critical_count': 0,
            'high_count': 0,
            'medium_count': 0,
            'low_count': 0,
            'test_phases': []
        }
        
        def count_vulns(data):
            if isinstance(data, dict):
                # V√©rifier si c'est une liste de vuln√©rabilit√©s
                if 'vulnerabilities' in data:
                    vulns = data['vulnerabilities']
                    
                    # S'assurer que c'est bien une liste
                    if isinstance(vulns, list):
                        for vuln in vulns:
                            # S'assurer que vuln est un dict
                            if isinstance(vuln, dict):
                                stats['total_vulnerabilities'] += 1
                                severity = vuln.get('severity', 'Medium')
                                
                                if severity == 'Critical':
                                    stats['critical_count'] += 1
                                elif severity == 'High':
                                    stats['high_count'] += 1
                                elif severity == 'Medium':
                                    stats['medium_count'] += 1
                                else:
                                    stats['low_count'] += 1
                    elif isinstance(vulns, int):
                        # Si c'est un compteur au lieu d'une liste
                        stats['total_vulnerabilities'] += vulns
                
                # Parcourir r√©cursivement
                for key, value in data.items():
                    if key != 'vulnerabilities':  # √âviter de recompter
                        count_vulns(value)
            elif isinstance(data, list):
                for item in data:
                    count_vulns(item)
        
        try:
            count_vulns(results)
        except Exception as e:
            logger.error(f"Error counting vulnerabilities: {e}")
            # Continuer avec des stats par d√©faut
        
        # D√©terminer la posture de s√©curit√©
        if stats['total_vulnerabilities'] == 0:
            stats['security_posture'] = "Excellent"
        elif stats['critical_count'] > 0:
            stats['security_posture'] = "Critical"
        elif stats['high_count'] > 0:
            stats['security_posture'] = "High Risk"
        else:
            stats['security_posture'] = "Moderate"
        
        # Lister les phases test√©es
        phase_names = {
            'prompt_injection': 'Prompt Injection Testing',
            'advanced_llm': 'Advanced LLM Attacks',
            'chained_attacks': 'Chained Attack Scenarios',
            'fuzzing': 'Input Fuzzing',
            'data_integrity': 'Data Integrity Testing',
            'authentication': 'Authentication Security',
            'performance': 'Performance & DoS Testing',
            'network_security': 'Network Security Testing',
            'compliance': 'Compliance Audit',
            'integration': 'Live Application Testing'
        }
        
        for phase_key, phase_name in phase_names.items():
            if phase_key in results:
                phase_result = results[phase_key]
                if isinstance(phase_result, dict):
                    stats['test_phases'].append({
                        'name': phase_name,
                        'key': phase_key,
                        'executed': not phase_result.get('skipped', False),
                        'has_error': 'error' in phase_result
                    })
        
        return stats
    
    @staticmethod
    def generate_executive_summary(results: Dict) -> str:
        """
        G√©n√©rer un r√©sum√© ex√©cutif (Markdown)
        
        Args:
            results: R√©sultats complets du pentest
        
        Returns:
            str: R√©sum√© ex√©cutif en markdown
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # Calculer les statistiques globales
        stats = PentestReportGenerator._calculate_statistics(results)
        
        report = f"""# üîí Penetration Testing Report
## Secure Financial Analysis Assistant

**Generated:** {timestamp}
**Test Type:** Comprehensive Security Assessment
**Scope:** LLM Security, Authentication, Data Integrity, API Security, Network Security, Compliance

---

## Executive Summary

This penetration test was conducted on the Secure Financial Analysis Assistant application,
focusing on security controls specific to AI/LLM systems, authentication mechanisms,
data integrity, API security, network configuration, and regulatory compliance.

### Key Findings

- **Total Vulnerabilities Found:** {stats['total_vulnerabilities']}
  - üî¥ Critical: {stats['critical_count']}
  - üü† High: {stats['high_count']}
  - üü° Medium: {stats['medium_count']}
  - üü¢ Low: {stats['low_count']}

### Security Posture

"""
        
        if stats['total_vulnerabilities'] == 0:
            report += "‚úÖ **Excellent** - No vulnerabilities detected\n\n"
            report += "The application demonstrates a strong security posture with all tested controls functioning as expected.\n\n"
        elif stats['critical_count'] > 0:
            report += f"üî¥ **Critical** - {stats['critical_count']} critical vulnerabilities require immediate attention\n\n"
            report += "**URGENT ACTION REQUIRED:** Critical security issues have been identified that could lead to system compromise.\n\n"
        elif stats['high_count'] > 0:
            report += f"üü† **High Risk** - {stats['high_count']} high-severity vulnerabilities identified\n\n"
            report += "**ACTION RECOMMENDED:** High-severity issues should be addressed within 7 days.\n\n"
        else:
            report += f"üü° **Moderate** - {stats['total_vulnerabilities']} vulnerabilities to address\n\n"
            report += "Review and remediate as appropriate based on risk assessment.\n\n"
        
        # Phases test√©es
        report += "### Test Coverage\n\n"
        report += "The following security testing phases were executed:\n\n"
        
        for phase in stats['test_phases']:
            if phase['executed']:
                report += f"- ‚úÖ {phase['name']}\n"
            else:
                report += f"- ‚è≠Ô∏è {phase['name']} (Skipped)\n"
        
        report += "\n"
        
        return report
    
    @staticmethod
    def generate_technical_findings(results: Dict) -> str:
        """
        G√©n√©rer les r√©sultats techniques d√©taill√©s (Markdown)
        
        Args:
            results: R√©sultats complets du pentest
        
        Returns:
            str: R√©sultats techniques en markdown
        """
        report = "## Technical Findings\n\n"
        report += "This section provides detailed technical results for each testing phase.\n\n"
        
        # 1. Prompt Injection Tests
        if 'prompt_injection' in results:
            report += "### 1. Prompt Injection Testing\n\n"
            pi_results = results['prompt_injection']
            
            if not isinstance(pi_results, dict) or 'error' in pi_results:
                report += "‚ö†Ô∏è Tests encountered errors\n\n"
            else:
                report += f"**Summary:**\n"
                report += f"- Total Tests: {pi_results.get('total_tests', 0)}\n"
                report += f"- Blocked Attacks: {pi_results.get('blocked', 0)}\n"
                report += f"- Successful Attacks: {pi_results.get('successful_attacks', 0)}\n"
                report += f"- Block Rate: {pi_results.get('block_rate', 0):.1f}%\n\n"
                
                if pi_results.get('successful_attacks', 0) > 0:
                    report += "‚ö†Ô∏è Some prompt injection attacks were not blocked.\n\n"
                else:
                    report += "‚úÖ All prompt injection attacks were successfully blocked.\n\n"
        
        # Ajouter les autres sections de tests
        if 'fuzzing' in results:
            report += "### 2. Input Fuzzing\n\n"
            fuzz_results = results['fuzzing']
            
            if isinstance(fuzz_results, dict) and 'ticker' in fuzz_results:
                ticker_res = fuzz_results['ticker']
                report += f"**Ticker Input Testing:**\n"
                report += f"- Total Tests: {ticker_res.get('total_tests', 0)}\n"
                report += f"- Block Rate: {ticker_res.get('block_rate', 0):.1f}%\n\n"
        
        if 'authentication' in results:
            report += "### 3. Authentication Security\n\n"
            auth_results = results['authentication']
            
            if isinstance(auth_results, dict):
                report += "Authentication mechanisms were tested including:\n"
                report += "- Password policy enforcement\n"
                report += "- Brute force protection\n"
                report += "- JWT security\n"
                report += "- Session management\n\n"
        
        return report
    
    @staticmethod
    def generate_mitre_analysis(mitre_results: Dict) -> str:
        """
        G√©n√©rer l'analyse MITRE ATT&CK (Markdown)
        
        Args:
            mitre_results: R√©sultats MITRE
        
        Returns:
            str: Analyse MITRE en markdown
        """
        report = "## MITRE ATT&CK Coverage\n\n"
        
        if not isinstance(mitre_results, dict):
            report += "‚ö†Ô∏è MITRE analysis not available\n\n"
            return report
        
        overall = mitre_results.get('overall', {})
        
        report += f"### Overall Coverage\n\n"
        report += f"- **Total Techniques Mapped:** {overall.get('total_techniques', 0)}\n"
        report += f"- **Tested:** {overall.get('tested', 0)} ({overall.get('test_coverage', 0):.1f}%)\n"
        report += f"- **Mitigated:** {overall.get('mitigated', 0)} ({overall.get('mitigation_coverage', 0):.1f}%)\n\n"
        
        tactics = mitre_results.get('tactics', {})
        
        if tactics:
            report += "### Coverage by Tactic\n\n"
            report += "| Tactic | Total | Tested | Mitigated | Test Coverage | Mitigation Coverage |\n"
            report += "|--------|-------|--------|-----------|---------------|---------------------|\n"
            
            for tactic_name, tactic_data in tactics.items():
                report += f"| {tactic_name} | {tactic_data.get('total', 0)} | {tactic_data.get('tested', 0)} | {tactic_data.get('mitigated', 0)} | {tactic_data.get('test_coverage', 0):.0f}% | {tactic_data.get('mitigation_coverage', 0):.0f}% |\n"
            
            report += "\n"
        
        return report
    
    @staticmethod
    def generate_recommendations(results: Dict) -> str:
        """
        G√©n√©rer les recommandations (Markdown)
        
        Args:
            results: R√©sultats complets du pentest
        
        Returns:
            str: Recommandations en markdown
        """
        report = "## Recommendations\n\n"
        report += "Based on the security assessment, the following actions are recommended:\n\n"
        
        stats = PentestReportGenerator._calculate_statistics(results)
        
        report += "### Priority Actions\n\n"
        
        if stats['critical_count'] > 0:
            report += "#### üî¥ Critical (Immediate Action Required)\n\n"
            report += "1. Address all critical vulnerabilities immediately\n"
            report += "2. Conduct emergency security review\n"
            report += "3. Consider temporary service restrictions until fixed\n\n"
        
        if stats['high_count'] > 0:
            report += "#### üü† High Priority (Within 7 Days)\n\n"
            report += "1. Review and remediate high-severity issues\n"
            report += "2. Implement additional security controls\n"
            report += "3. Schedule follow-up testing\n\n"
        
        report += "### General Recommendations\n\n"
        report += "1. Conduct regular security assessments (monthly/quarterly)\n"
        report += "2. Implement continuous security monitoring\n"
        report += "3. Maintain security awareness training for development team\n"
        report += "4. Keep all dependencies and frameworks up to date\n"
        report += "5. Follow OWASP Top 10 and LLM-specific security guidelines\n\n"
        
        return report
    
    @staticmethod
    def generate_json_report(results: Dict, mitre_results: Dict, console_output: str) -> Dict:
        """
        G√©n√©rer le rapport complet en format JSON
        
        Args:
            results: R√©sultats des tests
            mitre_results: R√©sultats MITRE
            console_output: Sortie console captur√©e
        
        Returns:
            Dict: Rapport structur√© en JSON
        """
        timestamp = datetime.now().isoformat()
        
        # Calculer les statistiques
        stats = PentestReportGenerator._calculate_statistics(results)
        
        # Construire le rapport JSON
        json_report = {
            "metadata": {
                "generated_at": timestamp,
                "test_type": "Comprehensive Security Assessment",
                "scope": [
                    "LLM Security",
                    "Authentication",
                    "Data Integrity",
                    "API Security",
                    "Network Security",
                    "Compliance"
                ],
                "version": "1.0.0",
                "tool": "Advanced Penetration Testing Suite"
            },
            "executive_summary": {
                "total_vulnerabilities": stats['total_vulnerabilities'],
                "critical_count": stats['critical_count'],
                "high_count": stats['high_count'],
                "medium_count": stats['medium_count'],
                "low_count": stats['low_count'],
                "security_posture": stats['security_posture'],
                "test_phases_executed": stats['test_phases']
            },
            "console_output": {
                "full_log": console_output if console_output else "",
                "log_lines": console_output.split('\n') if console_output else []
            },
            "test_results": results,
            "mitre_attack": mitre_results,
            "recommendations": PentestReportGenerator._generate_recommendations_json(results),
            "vulnerabilities_by_severity": {
                "critical": PentestReportGenerator._extract_vulnerabilities_by_severity(results, "Critical"),
                "high": PentestReportGenerator._extract_vulnerabilities_by_severity(results, "High"),
                "medium": PentestReportGenerator._extract_vulnerabilities_by_severity(results, "Medium"),
                "low": PentestReportGenerator._extract_vulnerabilities_by_severity(results, "Low")
            },
            "summary_statistics": {
                "total_tests_run": PentestReportGenerator._count_total_tests(results),
                "tests_passed": PentestReportGenerator._count_passed_tests(results),
                "tests_failed": PentestReportGenerator._count_failed_tests(results),
                "average_block_rate": PentestReportGenerator._calculate_avg_block_rate(results)
            }
        }
        
        return json_report
    
    @staticmethod
    def _generate_recommendations_json(results: Dict) -> List[Dict]:
        """G√©n√©rer les recommandations en format JSON"""
        recommendations = []
        
        # Analyser les r√©sultats pour g√©n√©rer des recommandations
        def analyze_vulns(data, path=""):
            if isinstance(data, dict):
                if 'vulnerabilities' in data and isinstance(data['vulnerabilities'], list):
                    for vuln in data['vulnerabilities']:
                        if isinstance(vuln, dict):
                            vuln_type = vuln.get('type', 'unknown')
                            
                            # Mapping des recommandations
                            if 'prompt' in vuln_type.lower() or 'injection' in vuln_type.lower():
                                recommendations.append({
                                    'priority': 'Critical',
                                    'title': 'Strengthen Input Validation',
                                    'description': 'Implement additional input validation and sanitization for LLM prompts',
                                    'impact': 'Prevents injection attacks and unauthorized access',
                                    'effort': 'Medium',
                                    'test_path': path
                                })
                            elif 'authentication' in vuln_type.lower():
                                recommendations.append({
                                    'priority': 'High',
                                    'title': 'Enhance Authentication Controls',
                                    'description': 'Strengthen authentication mechanisms',
                                    'impact': 'Prevents unauthorized access',
                                    'effort': 'Medium',
                                    'test_path': path
                                })
                
                for key, value in data.items():
                    if key != 'vulnerabilities':
                        new_path = f"{path}/{key}" if path else key
                        analyze_vulns(value, new_path)
            elif isinstance(data, list):
                for i, item in enumerate(data):
                    analyze_vulns(item, f"{path}[{i}]")
        
        try:
            analyze_vulns(results)
        except Exception as e:
            logger.error(f"Error generating recommendations: {e}")
        
        # D√©duplication
        unique_recs = {}
        for rec in recommendations:
            if rec['title'] not in unique_recs:
                unique_recs[rec['title']] = rec
        
        return list(unique_recs.values())
    
    @staticmethod
    def _extract_vulnerabilities_by_severity(results: Dict, severity: str) -> List[Dict]:
        """Extraire toutes les vuln√©rabilit√©s d'une s√©v√©rit√© donn√©e"""
        vulnerabilities = []
        
        def extract_vulns(data, path=""):
            if isinstance(data, dict):
                if 'vulnerabilities' in data and isinstance(data['vulnerabilities'], list):
                    for vuln in data['vulnerabilities']:
                        if isinstance(vuln, dict) and vuln.get('severity') == severity:
                            vuln_copy = vuln.copy()
                            vuln_copy['test_path'] = path
                            vulnerabilities.append(vuln_copy)
                
                for key, value in data.items():
                    if key != 'vulnerabilities':
                        new_path = f"{path}/{key}" if path else key
                        extract_vulns(value, new_path)
            elif isinstance(data, list):
                for i, item in enumerate(data):
                    extract_vulns(item, f"{path}[{i}]")
        
        try:
            extract_vulns(results)
        except Exception as e:
            logger.error(f"Error extracting vulnerabilities: {e}")
        
        return vulnerabilities
    
    @staticmethod
    def _count_total_tests(results: Dict) -> int:
        """Compter le nombre total de tests"""
        total = 0
        
        def count_tests(data):
            nonlocal total
            if isinstance(data, dict):
                if 'total_tests' in data and isinstance(data['total_tests'], int):
                    total += data['total_tests']
                for value in data.values():
                    count_tests(value)
            elif isinstance(data, list):
                for item in data:
                    count_tests(item)
        
        try:
            count_tests(results)
        except Exception as e:
            logger.error(f"Error counting tests: {e}")
        
        return total
    
    @staticmethod
    def _count_passed_tests(results: Dict) -> int:
        """Compter le nombre de tests r√©ussis"""
        passed = 0
        
        def count_passed(data):
            nonlocal passed
            if isinstance(data, dict):
                if 'blocked' in data and isinstance(data['blocked'], int):
                    passed += data['blocked']
                if 'passed' in data and isinstance(data['passed'], int):
                    passed += data['passed']
                for value in data.values():
                    count_passed(value)
            elif isinstance(data, list):
                for item in data:
                    count_passed(item)
        
        try:
            count_passed(results)
        except Exception as e:
            logger.error(f"Error counting passed tests: {e}")
        
        return passed
    
    @staticmethod
    def _count_failed_tests(results: Dict) -> int:
        """Compter le nombre de tests √©chou√©s"""
        failed = 0
        
        def count_failed(data):
            nonlocal failed
            if isinstance(data, dict):
                if 'successful_attacks' in data and isinstance(data['successful_attacks'], int):
                    failed += data['successful_attacks']
                if 'failed' in data and isinstance(data['failed'], int):
                    failed += data['failed']
                if 'vulnerabilities' in data and isinstance(data['vulnerabilities'], list):
                    failed += len(data['vulnerabilities'])
                for value in data.values():
                    count_failed(value)
            elif isinstance(data, list):
                for item in data:
                    count_failed(item)
        
        try:
            count_failed(results)
        except Exception as e:
            logger.error(f"Error counting failed tests: {e}")
        
        return failed
    
    @staticmethod
    def _calculate_avg_block_rate(results: Dict) -> float:
        """Calculer le taux de blocage moyen"""
        block_rates = []
        
        def collect_rates(data):
            if isinstance(data, dict):
                if 'block_rate' in data and isinstance(data['block_rate'], (int, float)):
                    block_rates.append(float(data['block_rate']))
                for value in data.values():
                    collect_rates(value)
            elif isinstance(data, list):
                for item in data:
                    collect_rates(item)
        
        try:
            collect_rates(results)
        except Exception as e:
            logger.error(f"Error calculating block rate: {e}")
        
        if block_rates:
            return sum(block_rates) / len(block_rates)
        return 0.0
    
    @staticmethod
    def save_reports(markdown_content: str, json_data: Dict, output_dir: str = "pentest_reports") -> Tuple[str, str]:
        """
        Sauvegarder les rapports MD et JSON
        
        Args:
            markdown_content: Contenu Markdown
            json_data: Donn√©es JSON
            output_dir: R√©pertoire de sortie
        
        Returns:
            Tuple[str, str]: Chemins des fichiers (MD, JSON)
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Sauvegarder le Markdown
        md_filename = f"pentest_report_{timestamp}.md"
        md_filepath = output_path / md_filename
        
        with open(md_filepath, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        logger.info(f"Markdown report saved to: {md_filepath}")
        
        # Sauvegarder le JSON
        json_filename = f"pentest_report_{timestamp}.json"
        json_filepath = output_path / json_filename
        
        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"JSON report saved to: {json_filepath}")
        
        return str(md_filepath), str(json_filepath)
    
    @classmethod
    def generate_full_report_with_console(cls, results: Dict, mitre_results: Dict, console_output: str) -> str:
        """
        G√©n√©rer le rapport complet Markdown avec sortie console
        
        Args:
            results: R√©sultats des tests
            mitre_results: R√©sultats MITRE
            console_output: Sortie console captur√©e
        
        Returns:
            str: Rapport complet en markdown
        """
        try:
            report = cls.generate_executive_summary(results)
            report += "\n---\n\n"
            report += cls.generate_console_output_section(console_output)
            report += "\n---\n\n"
            report += cls.generate_technical_findings(results)
            report += "\n---\n\n"
            report += cls.generate_mitre_analysis(mitre_results)
            report += "\n---\n\n"
            report += cls.generate_recommendations(results)
            
            report += "\n---\n\n"
            report += "## Report Information\n\n"
            report += f"- **Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            report += f"- **Framework:** MITRE ATT&CK + OWASP Top 10 + LLM Security\n"
            report += f"- **Tool:** Advanced Penetration Testing Suite v1.0\n"
            report += f"- **Formats:** Markdown (.md) + JSON (.json)\n"
            report += f"- **Complete Console Output:** Included in both formats\n\n"
            report += "---\n\n"
            report += "*This report contains sensitive security information and should be handled accordingly.*\n"
            
            return report
        except Exception as e:
            logger.error(f"Error generating full report: {e}")
            return f"# Error Generating Report\n\nAn error occurred: {str(e)}\n"